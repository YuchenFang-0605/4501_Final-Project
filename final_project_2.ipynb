{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a5c026",
   "metadata": {},
   "source": [
    "# Final Project - NYC Apartment Search - Group 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8272165",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a5841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shapely\n",
    "import sqlalchemy as db\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "from functools import partial\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ce7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"nzMxHq7vh4aj5sl0jx9gggdlP\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"resource/erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"resource/5rq2-4hqu.geojson\"\n",
    "\n",
    "DB_NAME = \"NYC-Data\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"dddd\"  # Replace with your actual PostgreSQL password\n",
    "DB_URL = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@localhost:5433/{DB_NAME}\"\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "DB_PORT = 5433\n",
    "\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f63ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c522d",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f50b34",
   "metadata": {},
   "source": [
    "+ The following code defines functions to download data , clean and filtering for the relevant data, fill in missing data, and generate samples of these datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ddfdd6",
   "metadata": {},
   "source": [
    "### 1.1 Data downloading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8f1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "\n",
    "def download_nyc_geojson_data(url, force=False):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    url_path = parsed_url.path.strip(\"/\")\n",
    "    \n",
    "    filename = DATA_DIR / url_path\n",
    "    \n",
    "    if force or not filename.exists():\n",
    "        print(f\"Downloading {url} to {filename}...\")\n",
    "\n",
    "        response = requests.get(url)\n",
    "        geojson_data = response.json()\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(geojson_data, f, default=str)\n",
    "        print(f\"Done downloading {url}.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading from {filename}...\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ab2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load and clean zipcode data from a file.\n",
    "\n",
    "    Parameters:\n",
    "        zipcode_datafile (str): The file path to the zipcode data file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing zipcode data.\n",
    "    \"\"\"\n",
    "    # Load the data from the file\n",
    "    df = gpd.read_file(zipcode_datafile)\n",
    "    \n",
    "    # To keep the necessary columns\n",
    "    df_selected = df\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    df_selected = df_selected.dropna()\n",
    "\n",
    "    # Convert column names to lowercase\n",
    "    df_selected.columns = [col.lower() for col in df_selected.columns]\n",
    "\n",
    "    # Convert column types (converting 'population' to int)\n",
    "    df_selected['population'] = df_selected['population'].astype(int)\n",
    "    \n",
    "    # drop duplicate zipcodes\n",
    "    df_selected = df_selected.drop_duplicates(subset='zipcode')\n",
    "    \n",
    "    # Change the SRID to a specific value (EPSG 4326 - WGS 84)\n",
    "    df_selected = df_selected.to_crs(epsg=4326)\n",
    "    \n",
    "    return df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d746d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data():\n",
    "    start_date = \"2015-01-01T00:00:00.000\"\n",
    "    end_date = \"2023-09-30T23:59:59.000\"\n",
    "\n",
    "    url = (\n",
    "        f\"{BASE_NYC_DATA_URL}{NYC_DATA_311}?\"\n",
    "        f\"$$app_token={NYC_DATA_APP_TOKEN}&\"\n",
    "        f\"$where=created_date between '{start_date}' and '{end_date}'&\"\n",
    "        f\"$limit=10000\"\n",
    "    )\n",
    "    \n",
    "    filename = download_nyc_geojson_data(url)\n",
    "    df=gpd.read_file(filename)\n",
    "    \n",
    "    # To keep the necessary columns\n",
    "    df_selected=df[['unique_key', 'created_date','city','incident_zip','complaint_type','geometry']]\n",
    "\n",
    "    # Drop rows with missing values---we find that if drop all na, it will delete all the data.\n",
    "    df_selected=df_selected.dropna()\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    df_selected.columns = [col.lower() for col in df_selected.columns]\n",
    "    \n",
    "    # Change the SRID to a specific value (EPSG 4326 - WGS 84)\n",
    "    df_selected['geometry'] = df_selected['geometry'].to_crs(epsg=4326)\n",
    "    \n",
    "    \n",
    "    return df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd44dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data():\n",
    "    url=f\"{BASE_NYC_DATA_URL}{NYC_DATA_TREES}?$$app_token={NYC_DATA_APP_TOKEN}&$limit=10000\"\n",
    "    filename=download_nyc_geojson_data(url)\n",
    "    df=gpd.read_file(filename)\n",
    "    \n",
    "    # To keep the necessary columns\n",
    "    df_selected=df[['tree_id','zipcode','address','health','zip_city','spc_common','status','sidewalk','borocode','block_id','geometry']]\n",
    "    \n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    df_selected=df_selected.dropna()\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    df_selected.columns = [col.lower() for col in df_selected.columns]\n",
    "    \n",
    "    # Change the SRID to a specific value (EPSG 4326 - WGS 84)\n",
    "    df_selected = df_selected.to_crs(epsg=4326)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb0bd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data():\n",
    "    \"\"\"\n",
    "    Load and clean Zillow data from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The file path to the Zillow data CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing Zillow data.\n",
    "    \"\"\"\n",
    "    # Load the data from the CSV file\n",
    "    zillow_data = pd.read_csv(ZILLOW_DATA_FILE)\n",
    "\n",
    "    # To keep the necessary columns\n",
    "    zillow_selected = zillow_data\n",
    "    \n",
    "    # only keep data in NY\n",
    "    zillow_selected = zillow_selected[zillow_selected['State'] == 'NY']\n",
    "    zillow_selected = zillow_selected.dropna()\n",
    "    \n",
    "    return zillow_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "637e3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = download_and_clean_311_data()\n",
    "    geodf_tree_data = download_and_clean_tree_data()\n",
    "    df_zillow_data = load_and_clean_zillow_data()\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685942c-26dc-40db-84c2-a71aa3340806",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ac6a3",
   "metadata": {},
   "source": [
    "+ This part is going to take the datasets downloaded & cleaned from Part 1, and populate a PostgreSQL database with tables generated from the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1ebb6",
   "metadata": {},
   "source": [
    "### 2.1 Creating new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f079fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_new_postgis_database(username, db_name):\n",
    "    conn = None\n",
    "\n",
    "    password = \"dddd\"\n",
    "\n",
    "    conn = psycopg2.connect(user=username, password=password, dbname='template1', connect_timeout=10)\n",
    "    conn.autocommit = True\n",
    "\n",
    "    # Check for existing transactions and commit or rollback\n",
    "    if conn.get_transaction_status() != psycopg2.extensions.TRANSACTION_STATUS_IDLE:\n",
    "        conn.rollback()\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        # Create a new database\n",
    "        create_db_command = sql.SQL(\"CREATE DATABASE {}\").format(sql.Identifier(db_name))\n",
    "        cursor.execute(create_db_command)\n",
    "\n",
    "    conn.autocommit = False\n",
    "    conn.close()\n",
    "    conn = psycopg2.connect(user=username, database=db_name, password=password, connect_timeout=10)\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        # Enable PostGIS extension\n",
    "        enable_postgis_command = \"CREATE EXTENSION IF NOT EXISTS postgis;\"\n",
    "        cursor.execute(enable_postgis_command)setup_new_postgis_database(DB_USER, DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "590ed80d-7b60-484f-a123-23b673b0f440",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateDatabase",
     "evalue": "错误:  数据库 \"NYCData2\" 已经存在\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateDatabase\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m setup_new_postgis_database(DB_USER, DB_NAME)\n",
      "Cell \u001b[1;32mIn[44], line 16\u001b[0m, in \u001b[0;36msetup_new_postgis_database\u001b[1;34m(username, db_name)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Create a new database\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     create_db_command \u001b[38;5;241m=\u001b[39m sql\u001b[38;5;241m.\u001b[39mSQL(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE DATABASE \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(sql\u001b[38;5;241m.\u001b[39mIdentifier(db_name))\n\u001b[1;32m---> 16\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(create_db_command)\n\u001b[0;32m     18\u001b[0m conn\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mDuplicateDatabase\u001b[0m: 错误:  数据库 \"NYCData2\" 已经存在\n"
     ]
    }
   ],
   "source": [
    "setup_new_postgis_database(DB_USER, DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb0349",
   "metadata": {},
   "source": [
    "### 2.2 Creating Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e06c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "CREATE TABLE ZIPCODE (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    zipcode VARCHAR(10) UNIQUE,\n",
    "    borough VARCHAR(50),\n",
    "    neighborhood VARCHAR(50),\n",
    "    population INT,\n",
    "    area FLOAT,\n",
    "    state VARCHAR(50),\n",
    "    county VARCHAR(50),\n",
    "    st_fips VARCHAR(10),\n",
    "    cty_fips VARCHAR(10),\n",
    "    url VARCHAR(255),\n",
    "    shape_area FLOAT,\n",
    "    shape_len FLOAT,\n",
    "    geometry GEOMETRY\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "CREATE TABLE NYC_311 (\n",
    "    unique_key VARCHAR(50) PRIMARY KEY,\n",
    "    created_date TIMESTAMP,\n",
    "    city VARCHAR(100),\n",
    "    incident_zip VARCHAR(10) REFERENCES ZIPCODE(zipcode),\n",
    "    complaint_type VARCHAR(100),\n",
    "    geometry GEOMETRY\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "CREATE TABLE NYC_TREE (\n",
    "    tree_id VARCHAR(50) PRIMARY KEY,\n",
    "    zipcode VARCHAR(10),\n",
    "    address VARCHAR(255),\n",
    "    health VARCHAR(255), \n",
    "    zip_city VARCHAR(255), \n",
    "    spc_common VARCHAR(255),\n",
    "    status VARCHAR(255),\n",
    "    sidewalk VARCHAR(255),\n",
    "    borocode VARCHAR(10),\n",
    "    block_id VARCHAR(255),\n",
    "    geometry GEOMETRY\n",
    ");\n",
    "\"\"\"\n",
    "# Get column names and their data types\n",
    "column_definitions = []\n",
    "for col, dtype in zip(df_zillow_data.columns, df_zillow_data.dtypes):\n",
    "    if dtype == 'object':\n",
    "        column_definitions.append(f'\"{col}\" VARCHAR(255)')\n",
    "    elif dtype == 'float64':\n",
    "        column_definitions.append(f'\"{col}\" FLOAT')\n",
    "    elif dtype == 'int64':\n",
    "        column_definitions.append(f'\"{col}\" INTEGER')\n",
    "\n",
    "# Join the column definitions\n",
    "columns_definition = ', '.join(column_definitions)\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "CREATE TABLE ZILLOW (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    \"\"\" + columns_definition + \"\"\"\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DB_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "    f.write(NYC_311_SCHEMA)\n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "    f.write(ZILLOW_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffb86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using SQL (as opposed to SQLAlchemy), execute the schema files to create tables\n",
    "with open(DB_SCHEMA_FILE, 'r') as file:\n",
    "        combined_schema_sql = file.read()\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Assuming combined_schema_sql contains multiple SQL statements separated by ';'\n",
    "sql_statements = combined_schema_sql.split(';')\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    for statement in sql_statements:\n",
    "        if statement.strip():  # Skip empty statements\n",
    "            connection.execute(text(statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1485f",
   "metadata": {},
   "source": [
    "### 2.3 Adding Data to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(tablename_to_dataframe):\n",
    "    # write INSERT statements or use pandas/geopandas to write SQL\n",
    "    for table_name, df in tablename_to_dataframe.items():\n",
    "        # Write the DataFrame to the PostgreSQL database\n",
    "        if 'geometry' in df.columns:\n",
    "            # If the DataFrame has a geometry column, use GeoAlchemy's `to_postgis`\n",
    "            df.to_postgis(table_name, con=engine, if_exists='replace', index=False)\n",
    "        else:\n",
    "            # If it doesn't have a geometry column, use the regular `to_sql`\n",
    "            df.to_sql(table_name, con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename_to_dataframe = {\n",
    "    \"zipcodes\": geodf_zipcode_data,\n",
    "    \"complaints\": geodf_311_data,\n",
    "    \"trees\": geodf_tree_data,\n",
    "    \"rents\": df_zillow_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(tablename_to_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
